Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 4191
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 5895
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 6412
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 1617
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 7817
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 2369
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 84
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 7244
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 495
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 5288
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 2926
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 7561
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 1141
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 3431
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 2362
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 9700
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 7144
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 7798
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 8493
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 4249
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 9763
Use SGD
Namespace(batch_size=32, dataset='modelnet40', epochs=200, eval=False, exp_name='ccS', gpu='1', lr=0.001, model_path='', momentum=0.9, no_cuda=False, num_points=1024, scheduler='cos', test_batch_size=16, use_sgd=True)
random seed is: 8348
Use SGD
Train 0, loss: 3.480389, train acc: 0.177117
Test 0, loss: 3.459497, test acc: 0.196921
best: 0.197
Train 1, loss: 3.109243, train acc: 0.281657
Test 1, loss: 2.768805, test acc: 0.380065
best: 0.380
Train 2, loss: 2.828886, train acc: 0.369809
Test 2, loss: 2.726051, test acc: 0.382091
best: 0.382
Train 3, loss: 2.675907, train acc: 0.418160
Test 3, loss: 2.629164, test acc: 0.420583
best: 0.421
Train 4, loss: 2.588361, train acc: 0.460301
Test 4, loss: 2.422806, test acc: 0.501216
best: 0.501
Train 5, loss: 2.504510, train acc: 0.490533
Test 5, loss: 2.429447, test acc: 0.488655
best: 0.501
Train 6, loss: 2.444929, train acc: 0.516287
Test 6, loss: 2.323083, test acc: 0.551053
best: 0.551
Train 7, loss: 2.400152, train acc: 0.533998
Test 7, loss: 2.206503, test acc: 0.609400
best: 0.609
Train 8, loss: 2.352318, train acc: 0.550896
Test 8, loss: 2.179515, test acc: 0.617099
best: 0.617
Train 9, loss: 2.316272, train acc: 0.564230
Test 9, loss: 2.126757, test acc: 0.632901
best: 0.633
Train 10, loss: 2.280275, train acc: 0.587744
Test 10, loss: 2.137374, test acc: 0.630065
best: 0.633
Train 11, loss: 2.255991, train acc: 0.599552
Test 11, loss: 2.114304, test acc: 0.650729
best: 0.651
Train 12, loss: 2.223849, train acc: 0.610953
Test 12, loss: 2.104877, test acc: 0.657212
best: 0.657
Train 13, loss: 2.210766, train acc: 0.610444
Test 13, loss: 2.075463, test acc: 0.667747
best: 0.668
Train 14, loss: 2.185964, train acc: 0.628461
Test 14, loss: 2.036691, test acc: 0.676256
best: 0.676
Train 15, loss: 2.166073, train acc: 0.640574
Test 15, loss: 2.053108, test acc: 0.674635
best: 0.676
Train 16, loss: 2.154893, train acc: 0.640676
Test 16, loss: 2.004974, test acc: 0.704619
best: 0.705
Train 17, loss: 2.130018, train acc: 0.651975
Test 17, loss: 1.989179, test acc: 0.715964
best: 0.716
Train 18, loss: 2.117191, train acc: 0.663477
Test 18, loss: 2.021607, test acc: 0.692869
best: 0.716
Train 19, loss: 2.108152, train acc: 0.667854
Test 19, loss: 1.947866, test acc: 0.721637
best: 0.722
Train 20, loss: 2.089693, train acc: 0.669788
Test 20, loss: 1.937598, test acc: 0.745948
best: 0.746
Train 21, loss: 2.079948, train acc: 0.682716
Test 21, loss: 1.959410, test acc: 0.732577
best: 0.746
Train 22, loss: 2.073907, train acc: 0.679764
Test 22, loss: 1.977204, test acc: 0.721232
best: 0.746
Train 23, loss: 2.047947, train acc: 0.690961
Test 23, loss: 1.926727, test acc: 0.729741
best: 0.746
Train 24, loss: 2.053210, train acc: 0.685973
Test 24, loss: 1.894868, test acc: 0.756078
best: 0.756
Train 25, loss: 2.029618, train acc: 0.695033
Test 25, loss: 1.936217, test acc: 0.727310
best: 0.756
Train 26, loss: 2.027286, train acc: 0.701140
Test 26, loss: 1.880897, test acc: 0.758104
best: 0.758
Train 27, loss: 2.016353, train acc: 0.704397
Test 27, loss: 1.927810, test acc: 0.726094
best: 0.758
Train 28, loss: 2.019142, train acc: 0.704194
Test 28, loss: 1.891551, test acc: 0.758914
best: 0.759
Train 29, loss: 2.006010, train acc: 0.702667
Test 29, loss: 1.830458, test acc: 0.786062
best: 0.786
Train 30, loss: 1.997986, train acc: 0.712744
Test 30, loss: 1.882532, test acc: 0.754862
best: 0.786
Train 31, loss: 1.990039, train acc: 0.713660
Test 31, loss: 1.853712, test acc: 0.774311
best: 0.786
Train 32, loss: 1.970384, train acc: 0.725570
Test 32, loss: 1.872096, test acc: 0.751621
best: 0.786
Train 33, loss: 1.977200, train acc: 0.721091
Test 33, loss: 1.849856, test acc: 0.768233
best: 0.786
Train 34, loss: 1.972138, train acc: 0.726792
Test 34, loss: 1.836734, test acc: 0.782415
best: 0.786
Train 35, loss: 1.956323, train acc: 0.733917
Test 35, loss: 1.857702, test acc: 0.775932
best: 0.786
Train 36, loss: 1.958009, train acc: 0.731779
Test 36, loss: 1.820106, test acc: 0.784441
best: 0.786
Train 37, loss: 1.952837, train acc: 0.729540
Test 37, loss: 1.835372, test acc: 0.775527
best: 0.786
Train 38, loss: 1.944839, train acc: 0.741653
Test 38, loss: 1.854144, test acc: 0.768233
best: 0.786
Train 39, loss: 1.939515, train acc: 0.739923
Test 39, loss: 1.806250, test acc: 0.796191
best: 0.796
Train 40, loss: 1.928174, train acc: 0.746437
Test 40, loss: 1.832707, test acc: 0.781199
best: 0.796
Train 41, loss: 1.930618, train acc: 0.742162
Test 41, loss: 1.819320, test acc: 0.782010
best: 0.796
Train 42, loss: 1.934042, train acc: 0.736360
Test 42, loss: 1.834258, test acc: 0.777553
best: 0.796
Train 43, loss: 1.914145, train acc: 0.753970
Test 43, loss: 1.829561, test acc: 0.786872
best: 0.796
Train 44, loss: 1.897130, train acc: 0.761808
Test 44, loss: 1.845890, test acc: 0.773096
best: 0.796
Train 45, loss: 1.907829, train acc: 0.753054
Test 45, loss: 1.805988, test acc: 0.790924
best: 0.796
Train 46, loss: 1.889837, train acc: 0.762622
Test 46, loss: 1.810706, test acc: 0.783225
best: 0.796
Train 47, loss: 1.895229, train acc: 0.765065
Test 47, loss: 1.816166, test acc: 0.783630
best: 0.796
Train 48, loss: 1.891774, train acc: 0.760688
Test 48, loss: 1.837238, test acc: 0.778768
best: 0.796
Train 49, loss: 1.886635, train acc: 0.764760
Test 49, loss: 1.834912, test acc: 0.766207
best: 0.796
Train 50, loss: 1.883712, train acc: 0.764251
Test 50, loss: 1.796587, test acc: 0.791734
best: 0.796
Train 51, loss: 1.875970, train acc: 0.768628
Test 51, loss: 1.787894, test acc: 0.789303
best: 0.796
Train 52, loss: 1.870208, train acc: 0.768424
Test 52, loss: 1.774224, test acc: 0.802269
best: 0.802
Train 53, loss: 1.876727, train acc: 0.766592
Test 53, loss: 1.768709, test acc: 0.813209
best: 0.813
Train 54, loss: 1.858277, train acc: 0.772292
Test 54, loss: 1.789254, test acc: 0.805511
best: 0.813
Train 55, loss: 1.861832, train acc: 0.777178
Test 55, loss: 1.794073, test acc: 0.796191
best: 0.813
Train 56, loss: 1.857140, train acc: 0.774634
Test 56, loss: 1.787942, test acc: 0.803079
best: 0.813
Train 57, loss: 1.849611, train acc: 0.777993
Test 57, loss: 1.751676, test acc: 0.822123
best: 0.822
Train 58, loss: 1.844794, train acc: 0.781250
Test 58, loss: 1.781981, test acc: 0.803890
best: 0.822
Train 59, loss: 1.834132, train acc: 0.784609
Test 59, loss: 1.790280, test acc: 0.797812
best: 0.822
Train 60, loss: 1.837009, train acc: 0.786034
Test 60, loss: 1.726900, test acc: 0.837520
best: 0.838
Train 61, loss: 1.831226, train acc: 0.788986
Test 61, loss: 1.765443, test acc: 0.809562
best: 0.838
Train 62, loss: 1.823859, train acc: 0.792040
Test 62, loss: 1.768039, test acc: 0.808752
best: 0.838
Train 63, loss: 1.822275, train acc: 0.788172
Test 63, loss: 1.774991, test acc: 0.807131
best: 0.838
Train 64, loss: 1.824107, train acc: 0.791124
Test 64, loss: 1.755265, test acc: 0.814019
best: 0.838
Train 65, loss: 1.812279, train acc: 0.798046
Test 65, loss: 1.766766, test acc: 0.807131
best: 0.838
Train 66, loss: 1.816545, train acc: 0.794890
Test 66, loss: 1.792269, test acc: 0.799028
best: 0.838
Train 67, loss: 1.800902, train acc: 0.804662
Test 67, loss: 1.770581, test acc: 0.797812
best: 0.838
Train 68, loss: 1.798890, train acc: 0.800590
Test 68, loss: 1.745555, test acc: 0.816045
best: 0.838
Train 69, loss: 1.800362, train acc: 0.802524
Test 69, loss: 1.718474, test acc: 0.833468
best: 0.838
Train 70, loss: 1.790230, train acc: 0.804662
Test 70, loss: 1.722253, test acc: 0.822123
best: 0.838
Train 71, loss: 1.787059, train acc: 0.812093
Test 71, loss: 1.713535, test acc: 0.841977
best: 0.842
Train 72, loss: 1.788067, train acc: 0.804051
Test 72, loss: 1.748799, test acc: 0.815235
best: 0.842
Train 73, loss: 1.779876, train acc: 0.811889
Test 73, loss: 1.728450, test acc: 0.830632
best: 0.842
Train 74, loss: 1.779528, train acc: 0.813518
Test 74, loss: 1.734680, test acc: 0.826985
best: 0.842
Train 75, loss: 1.774006, train acc: 0.812805
Test 75, loss: 1.696697, test acc: 0.840357
best: 0.842
Train 76, loss: 1.770042, train acc: 0.817081
Test 76, loss: 1.735035, test acc: 0.817261
best: 0.842
Train 77, loss: 1.770970, train acc: 0.812093
Test 77, loss: 1.713250, test acc: 0.835494
best: 0.842
Train 78, loss: 1.771065, train acc: 0.813518
Test 78, loss: 1.707012, test acc: 0.835494
best: 0.842
Train 79, loss: 1.756421, train acc: 0.820338
Test 79, loss: 1.725808, test acc: 0.834684
best: 0.842
Train 80, loss: 1.758839, train acc: 0.819524
Test 80, loss: 1.727784, test acc: 0.820097
best: 0.842
Train 81, loss: 1.757083, train acc: 0.820847
Test 81, loss: 1.707510, test acc: 0.829011
best: 0.842
Train 82, loss: 1.755697, train acc: 0.820745
Test 82, loss: 1.683818, test acc: 0.837925
best: 0.842
Train 83, loss: 1.746646, train acc: 0.827056
Test 83, loss: 1.694717, test acc: 0.841572
best: 0.842
Train 84, loss: 1.744372, train acc: 0.825122
Test 84, loss: 1.688254, test acc: 0.845219
best: 0.845
Train 85, loss: 1.744043, train acc: 0.824511
Test 85, loss: 1.682748, test acc: 0.850486
best: 0.850
Train 86, loss: 1.731686, train acc: 0.830008
Test 86, loss: 1.688403, test acc: 0.849271
best: 0.850
Train 87, loss: 1.727946, train acc: 0.833571
Test 87, loss: 1.710908, test acc: 0.841977
best: 0.850
Train 88, loss: 1.723479, train acc: 0.834691
Test 88, loss: 1.676781, test acc: 0.847650
best: 0.850
Train 89, loss: 1.719305, train acc: 0.831433
Test 89, loss: 1.676168, test acc: 0.849271
best: 0.850
Train 90, loss: 1.720078, train acc: 0.837541
Test 90, loss: 1.693002, test acc: 0.840357
best: 0.850
Train 91, loss: 1.721864, train acc: 0.832655
Test 91, loss: 1.676160, test acc: 0.846029
best: 0.850
Train 92, loss: 1.711228, train acc: 0.838660
Test 92, loss: 1.672926, test acc: 0.853323
best: 0.853
Train 93, loss: 1.719806, train acc: 0.835098
Test 93, loss: 1.689677, test acc: 0.839141
best: 0.853
Train 94, loss: 1.706636, train acc: 0.845277
Test 94, loss: 1.652838, test acc: 0.859400
best: 0.859
Train 95, loss: 1.699590, train acc: 0.840594
Test 95, loss: 1.667483, test acc: 0.846840
best: 0.859
Train 96, loss: 1.697983, train acc: 0.843750
Test 96, loss: 1.657249, test acc: 0.856564
best: 0.859
Train 97, loss: 1.693328, train acc: 0.847720
Test 97, loss: 1.674718, test acc: 0.844814
best: 0.859
Train 98, loss: 1.689817, train acc: 0.851079
Test 98, loss: 1.675099, test acc: 0.851702
best: 0.859
Train 99, loss: 1.685596, train acc: 0.851690
Test 99, loss: 1.670592, test acc: 0.850081
best: 0.859
Train 100, loss: 1.681776, train acc: 0.851181
Test 100, loss: 1.672622, test acc: 0.844003
best: 0.859
Train 101, loss: 1.671429, train acc: 0.856474
Test 101, loss: 1.663321, test acc: 0.845624
best: 0.859
Train 102, loss: 1.673162, train acc: 0.856881
Test 102, loss: 1.653951, test acc: 0.844408
best: 0.859
Train 103, loss: 1.666097, train acc: 0.859731
Test 103, loss: 1.660750, test acc: 0.847650
best: 0.859
Train 104, loss: 1.663879, train acc: 0.859833
Test 104, loss: 1.652883, test acc: 0.850081
best: 0.859
Train 105, loss: 1.665405, train acc: 0.854743
Test 105, loss: 1.658061, test acc: 0.860211
best: 0.860
Train 106, loss: 1.655914, train acc: 0.862989
Test 106, loss: 1.656147, test acc: 0.852917
best: 0.860
Train 107, loss: 1.658400, train acc: 0.862378
Test 107, loss: 1.653820, test acc: 0.851702
best: 0.860
Train 108, loss: 1.649739, train acc: 0.863090
Test 108, loss: 1.638956, test acc: 0.859400
best: 0.860
Train 109, loss: 1.648170, train acc: 0.863396
Test 109, loss: 1.647539, test acc: 0.852917
best: 0.860
Train 110, loss: 1.643769, train acc: 0.871234
Test 110, loss: 1.638657, test acc: 0.856564
best: 0.860
Train 111, loss: 1.646571, train acc: 0.864007
Test 111, loss: 1.660902, test acc: 0.845219
best: 0.860
Train 112, loss: 1.632955, train acc: 0.871743
Test 112, loss: 1.627486, test acc: 0.856159
best: 0.860
Train 113, loss: 1.627691, train acc: 0.875000
Test 113, loss: 1.649672, test acc: 0.857374
best: 0.860
Train 114, loss: 1.636117, train acc: 0.870521
Test 114, loss: 1.644092, test acc: 0.848055
best: 0.860
Train 115, loss: 1.632835, train acc: 0.873982
Test 115, loss: 1.636846, test acc: 0.854538
best: 0.860
Train 116, loss: 1.622461, train acc: 0.879581
Test 116, loss: 1.620801, test acc: 0.869530
best: 0.870
Train 117, loss: 1.619311, train acc: 0.876527
Test 117, loss: 1.638138, test acc: 0.854943
best: 0.870
Train 118, loss: 1.618117, train acc: 0.879377
Test 118, loss: 1.622171, test acc: 0.861021
best: 0.870
Train 119, loss: 1.605180, train acc: 0.887520
Test 119, loss: 1.624371, test acc: 0.864668
best: 0.870
Train 120, loss: 1.606056, train acc: 0.885993
Test 120, loss: 1.628554, test acc: 0.854943
best: 0.870
Train 121, loss: 1.608009, train acc: 0.885993
Test 121, loss: 1.622205, test acc: 0.869530
best: 0.870
Train 122, loss: 1.603667, train acc: 0.883245
Test 122, loss: 1.620477, test acc: 0.859806
best: 0.870
Train 123, loss: 1.599538, train acc: 0.884976
Test 123, loss: 1.620145, test acc: 0.861831
best: 0.870
Train 124, loss: 1.593845, train acc: 0.891796
Test 124, loss: 1.616880, test acc: 0.864668
best: 0.870
Train 125, loss: 1.589853, train acc: 0.887724
Test 125, loss: 1.608409, test acc: 0.869530
best: 0.870
Train 126, loss: 1.589027, train acc: 0.893831
Test 126, loss: 1.610318, test acc: 0.867099
best: 0.870
Train 127, loss: 1.587615, train acc: 0.888742
Test 127, loss: 1.619939, test acc: 0.858590
best: 0.870
Train 128, loss: 1.580117, train acc: 0.895053
Test 128, loss: 1.612961, test acc: 0.863857
best: 0.870
Train 129, loss: 1.576890, train acc: 0.894849
Test 129, loss: 1.623193, test acc: 0.861021
best: 0.870
Train 130, loss: 1.574927, train acc: 0.898616
Test 130, loss: 1.604827, test acc: 0.870746
best: 0.871
Train 131, loss: 1.570343, train acc: 0.900550
Test 131, loss: 1.612854, test acc: 0.861831
best: 0.871
Train 132, loss: 1.563257, train acc: 0.901568
Test 132, loss: 1.608768, test acc: 0.866694
best: 0.871
Train 133, loss: 1.568313, train acc: 0.900957
Test 133, loss: 1.620526, test acc: 0.858590
best: 0.871
Train 134, loss: 1.559964, train acc: 0.901669
Test 134, loss: 1.604464, test acc: 0.865883
best: 0.871
Train 135, loss: 1.558128, train acc: 0.904316
Test 135, loss: 1.613091, test acc: 0.857374
best: 0.871
Train 136, loss: 1.558455, train acc: 0.903400
Test 136, loss: 1.608355, test acc: 0.862237
best: 0.871
Train 137, loss: 1.556642, train acc: 0.903400
Test 137, loss: 1.592784, test acc: 0.869935
best: 0.871
Train 138, loss: 1.547447, train acc: 0.908693
Test 138, loss: 1.600161, test acc: 0.869125
best: 0.871
Train 139, loss: 1.546538, train acc: 0.906963
Test 139, loss: 1.598583, test acc: 0.867504
best: 0.871
Train 140, loss: 1.539090, train acc: 0.916633
Test 140, loss: 1.580529, test acc: 0.879660
best: 0.880
Train 141, loss: 1.539070, train acc: 0.915004
Test 141, loss: 1.584051, test acc: 0.874392
best: 0.880
Train 142, loss: 1.535658, train acc: 0.910525
Test 142, loss: 1.577849, test acc: 0.874797
best: 0.880
Train 143, loss: 1.525648, train acc: 0.919279
Test 143, loss: 1.587747, test acc: 0.878039
best: 0.880
Train 144, loss: 1.524607, train acc: 0.917447
Test 144, loss: 1.594884, test acc: 0.867909
best: 0.880
Train 145, loss: 1.528371, train acc: 0.920094
Test 145, loss: 1.586128, test acc: 0.871556
best: 0.880
Train 146, loss: 1.518443, train acc: 0.919076
Test 146, loss: 1.581994, test acc: 0.876013
best: 0.880
Train 147, loss: 1.520867, train acc: 0.919890
Test 147, loss: 1.590338, test acc: 0.869125
best: 0.880
Train 148, loss: 1.515338, train acc: 0.922740
Test 148, loss: 1.592103, test acc: 0.868314
best: 0.880
Train 149, loss: 1.511889, train acc: 0.924165
Test 149, loss: 1.576383, test acc: 0.876013
best: 0.880
Train 150, loss: 1.514397, train acc: 0.921213
Test 150, loss: 1.581330, test acc: 0.870746
best: 0.880
Train 151, loss: 1.510417, train acc: 0.925387
Test 151, loss: 1.573800, test acc: 0.878849
best: 0.880
Train 152, loss: 1.506305, train acc: 0.925692
Test 152, loss: 1.572016, test acc: 0.880875
best: 0.881
Train 153, loss: 1.501837, train acc: 0.926812
Test 153, loss: 1.571699, test acc: 0.876823
best: 0.881
Train 154, loss: 1.497998, train acc: 0.930273
Test 154, loss: 1.579315, test acc: 0.873582
best: 0.881
Train 155, loss: 1.498679, train acc: 0.926405
Test 155, loss: 1.573320, test acc: 0.870340
best: 0.881
Train 156, loss: 1.490735, train acc: 0.936279
Test 156, loss: 1.568255, test acc: 0.877229
best: 0.881
Train 157, loss: 1.495129, train acc: 0.928746
Test 157, loss: 1.566348, test acc: 0.875608
best: 0.881
Train 158, loss: 1.496715, train acc: 0.929051
Test 158, loss: 1.568008, test acc: 0.874797
best: 0.881
Train 159, loss: 1.489905, train acc: 0.934344
Test 159, loss: 1.567623, test acc: 0.876418
best: 0.881
Train 160, loss: 1.481164, train acc: 0.937093
Test 160, loss: 1.563860, test acc: 0.879660
best: 0.881
Train 161, loss: 1.484349, train acc: 0.937195
Test 161, loss: 1.566907, test acc: 0.877229
best: 0.881
Train 162, loss: 1.478907, train acc: 0.939129
Test 162, loss: 1.565529, test acc: 0.874392
best: 0.881
Train 163, loss: 1.480471, train acc: 0.940147
Test 163, loss: 1.557920, test acc: 0.878849
best: 0.881
Train 164, loss: 1.475935, train acc: 0.940961
Test 164, loss: 1.560260, test acc: 0.880470
best: 0.881
Train 165, loss: 1.472151, train acc: 0.942895
Test 165, loss: 1.560803, test acc: 0.879660
best: 0.881
Train 166, loss: 1.472295, train acc: 0.939332
Test 166, loss: 1.559267, test acc: 0.880470
best: 0.881
Train 167, loss: 1.467633, train acc: 0.943506
Test 167, loss: 1.562002, test acc: 0.878444
best: 0.881
Train 168, loss: 1.468194, train acc: 0.940656
Test 168, loss: 1.557224, test acc: 0.884522
best: 0.885
Train 169, loss: 1.464929, train acc: 0.943913
Test 169, loss: 1.565200, test acc: 0.877634
best: 0.885
Train 170, loss: 1.462151, train acc: 0.945033
Test 170, loss: 1.556064, test acc: 0.877229
best: 0.885
Train 171, loss: 1.460002, train acc: 0.949104
Test 171, loss: 1.555208, test acc: 0.881280
best: 0.885
Train 172, loss: 1.460776, train acc: 0.945440
Test 172, loss: 1.557115, test acc: 0.878039
best: 0.885
Train 173, loss: 1.457393, train acc: 0.948290
Test 173, loss: 1.560867, test acc: 0.873582
best: 0.885
Train 174, loss: 1.455104, train acc: 0.947781
Test 174, loss: 1.560995, test acc: 0.876418
best: 0.885
Train 175, loss: 1.452313, train acc: 0.949919
Test 175, loss: 1.558954, test acc: 0.874392
best: 0.885
Train 176, loss: 1.451084, train acc: 0.949715
Test 176, loss: 1.557526, test acc: 0.876013
best: 0.885
Train 177, loss: 1.449773, train acc: 0.951344
Test 177, loss: 1.557655, test acc: 0.884522
best: 0.885
Train 178, loss: 1.450433, train acc: 0.950733
Test 178, loss: 1.557255, test acc: 0.878849
best: 0.885
Train 179, loss: 1.445716, train acc: 0.951853
Test 179, loss: 1.556025, test acc: 0.880470
best: 0.885
Train 180, loss: 1.447302, train acc: 0.951445
Test 180, loss: 1.552322, test acc: 0.881280
best: 0.885
Train 181, loss: 1.443963, train acc: 0.952260
Test 181, loss: 1.556523, test acc: 0.884117
best: 0.885
Train 182, loss: 1.444580, train acc: 0.950428
Test 182, loss: 1.552178, test acc: 0.883306
best: 0.885
Train 183, loss: 1.443458, train acc: 0.952463
Test 183, loss: 1.550708, test acc: 0.884522
best: 0.885
Train 184, loss: 1.441549, train acc: 0.951649
Test 184, loss: 1.554731, test acc: 0.881686
best: 0.885
Train 185, loss: 1.439564, train acc: 0.956739
Test 185, loss: 1.550258, test acc: 0.886548
best: 0.887
Train 186, loss: 1.438983, train acc: 0.954397
Test 186, loss: 1.547986, test acc: 0.881686
best: 0.887
Train 187, loss: 1.440610, train acc: 0.955517
Test 187, loss: 1.548016, test acc: 0.884522
best: 0.887
Train 188, loss: 1.438083, train acc: 0.954397
Test 188, loss: 1.549004, test acc: 0.884117
best: 0.887
Train 189, loss: 1.437859, train acc: 0.956026
Test 189, loss: 1.550752, test acc: 0.878849
best: 0.887
Train 190, loss: 1.434774, train acc: 0.957553
Test 190, loss: 1.549677, test acc: 0.883712
best: 0.887
Train 191, loss: 1.435501, train acc: 0.958265
Test 191, loss: 1.546338, test acc: 0.881686
best: 0.887
Train 192, loss: 1.437528, train acc: 0.954805
Test 192, loss: 1.550634, test acc: 0.880470
best: 0.887
Train 193, loss: 1.434486, train acc: 0.957553
Test 193, loss: 1.550308, test acc: 0.882091
best: 0.887
Train 194, loss: 1.437179, train acc: 0.954194
Test 194, loss: 1.548679, test acc: 0.884522
best: 0.887
Train 195, loss: 1.434647, train acc: 0.955822
Test 195, loss: 1.547654, test acc: 0.884522
best: 0.887
Train 196, loss: 1.432532, train acc: 0.957655
Test 196, loss: 1.547502, test acc: 0.880875
best: 0.887
Train 197, loss: 1.435287, train acc: 0.956739
Test 197, loss: 1.548697, test acc: 0.881686
best: 0.887
Train 198, loss: 1.434105, train acc: 0.956331
Test 198, loss: 1.545804, test acc: 0.883306
best: 0.887
Train 199, loss: 1.432910, train acc: 0.955619
Test 199, loss: 1.548686, test acc: 0.880065
best: 0.887
